import os
import yaml
import hashlib
import subprocess
import logging
import shutil
import re
import ipaddress
import json
from datetime import datetime, timedelta, timezone
from collections import defaultdict

# --- å…¨å±€é…ç½® ---
# âš¡ï¸ v3.0 æœ€ç»ˆå®Œæ•´ç‰ˆ: ä¿®å¤å‘½åæŠ¢å  + ä¿®å¤æ—¶é—´Bug + å…¨å¥—UIç¾åŒ–
GENERATOR_VERSION = "v3.0_FINAL_COMPLETE" 
SOURCE_DIR = "temp_source/rule/Clash"
TARGET_DIR_MIHOMO = "rule/Mihomo"
TARGET_DIR_LOON = "rule/Loon"
HISTORY_FILE = "history.json"
README_FILE = "README.md"
MIHOMO_BIN = "./mihomo"

# ğŸ›‘ å˜ä½“å‰”é™¤é»‘åå•
IGNORE_KEYWORDS = ["Classical", "Domain", "For_Clash", "No_Resolve", "Clash"]

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger("DigitalArchitect")

filename_registry = {}

# --- å…³é”®è¯æ•‘æ´å­—å…¸ ---
KEYWORD_RESCUE_MAP = {
    "googlevideo": ["googlevideo.com"],
    "youtube": ["youtube.com", "ytimg.com"],
    "google": ["google.com", "googleapis.com"],
    "github": ["github.com", "githubusercontent.com"],
    "twitter": ["twitter.com", "t.co", "twimg.com"],
    "telegram": ["telegram.org", "t.me"],
    "netflix": ["netflix.com", "nflxvideo.net"],
    "facebook": ["facebook.com", "fbcdn.net"],
    "instagram": ["instagram.com", "cdninstagram.com"],
    "openai": ["openai.com"],
    "chatgpt": ["chatgpt.com", "oaistatic.com", "oaiusercontent.com"],
    "steam": ["steampowered.com", "steamcommunity.com"],
    "xbox": ["xbox.com", "xboxlive.com"],
    "microsoft": ["microsoft.com", "azure.com"]
}

# --- åŠ¨æ€å…ƒæ•°æ®è·å– ---
def get_metadata():
    repo_full = os.getenv('GITHUB_REPOSITORY')
    if repo_full and "/" in repo_full:
        author = repo_full.split("/")[0]
        raw_base = f"https://raw.githubusercontent.com/{repo_full}/main"
        repo_url = f"https://github.com/{repo_full}"
    else:
        author = "Unknown"
        raw_base = "https://raw.githubusercontent.com/Unknown/Test/main"
        repo_url = "https://github.com/Unknown/Test"
        repo_full = "YourName/RepoName"
    return author, raw_base, repo_url, repo_full

AUTHOR_NAME, RAW_BASE_URL, REPO_URL, REPO_NAME_DISPLAY = get_metadata()

# --- æ ¸å¿ƒç»„ä»¶ç±» ---
class KernelIntrospector:
    def __init__(self, bin_path):
        self.bin_path = bin_path
        if not os.path.exists(bin_path): raise FileNotFoundError(f"å†…æ ¸ç¼ºå¤±: {bin_path}")
        self.needs_format_arg = self._detect()
    def _detect(self):
        try:
            res = subprocess.run([self.bin_path, "convert-ruleset"], capture_output=True, text=True, timeout=5)
            out = res.stderr + res.stdout
            return "<format>" in out or " [format] " in out
        except: return False
    def get_cmd(self, behavior, src, dst):
        cmd = [self.bin_path, "convert-ruleset", behavior]
        if self.needs_format_arg: cmd.append("yaml")
        cmd.append(src)
        cmd.append(dst)
        return cmd

class RuleSet:
    def __init__(self):
        self.domain_entries = set()
        self.ip_entries = defaultdict(bool)
    def add_domain(self, line):
        line = line.strip().strip("'").strip('"')
        if not line or line.startswith('#'): return
        rule_type = "DOMAIN-SUFFIX"
        value = line
        if ',' in line:
            parts = line.split(',')
            if len(parts) >= 2:
                t, v = parts[0].upper().strip(), parts[1].strip()
                if 'DOMAIN' in t: rule_type, value = t, v
        if len(value) > 2: self.domain_entries.add((rule_type, value))
    def add_ip(self, line):
        if not line: return
        clean = line.replace("'", "").replace('"', "").strip()
        parts = re.split(r'[,\s]+', clean)
        target = None
        no_res = False
        for p in parts:
            p = p.strip()
            if not p or 'IP-' in p.upper(): continue
            if p.lower() == 'no-resolve': 
                no_res = True
                continue
            try:
                ipaddress.ip_network(p, strict=False)
                target = p
            except ValueError: continue
        if target:
            if not self.ip_entries[target]: self.ip_entries[target] = no_res

class HistoryManager:
    def __init__(self):
        self.history = {}
        if os.path.exists(HISTORY_FILE):
            try:
                with open(HISTORY_FILE, 'r') as f: self.history = json.load(f)
            except: pass
        self.current_time = int(datetime.now().timestamp())
    def get_file_hash(self, filepath):
        if not filepath or not os.path.exists(filepath): return ""
        with open(filepath, 'rb') as f: return hashlib.md5(f.read()).hexdigest()
        
    def should_skip(self, name, source_path, expected_files):
        src_hash = self.get_file_hash(source_path)
        if not src_hash: return False, ""
        
        record = self.history.get(name, {})
        last_hash = record.get('src_hash', "")
        
        # ğŸŸ¢ ä¿®å¤ Bug: ä»…å½“æºæ–‡ä»¶ Hash å˜åŒ–æ—¶æ‰é‡ç¼–ï¼Œå¿½ç•¥è„šæœ¬ç‰ˆæœ¬å˜åŒ–
        if src_hash != last_hash:
            return False, src_hash
            
        for f in expected_files:
            if not os.path.exists(f) or os.path.getsize(f) == 0:
                return False, src_hash
                
        return True, src_hash

    def update_record(self, name, src_hash):
        self.history[name] = {
            'src_hash': src_hash,
            'updated_at': self.current_time,
            'gen_ver': GENERATOR_VERSION
        }
    def get_days_ago(self, name):
        record = self.history.get(name, {})
        last_ts = record.get('updated_at', self.current_time)
        diff = datetime.fromtimestamp(self.current_time) - datetime.fromtimestamp(last_ts)
        return diff.days
    def save(self):
        with open(HISTORY_FILE, 'w') as f: json.dump(self.history, f, indent=2)

def get_smart_filename(rel_path):
    parts = rel_path.split(os.sep)
    base = os.path.splitext(parts[-1])[0]
    cand = base
    stack = parts[:-1]
    while cand in filename_registry:
        if filename_registry[cand] == rel_path: return cand
        if not stack:
            cand = f"{cand}_{hashlib.md5(rel_path.encode()).hexdigest()[:4]}"
            break
        cand = f"{stack.pop()}_{cand}"
    filename_registry[cand] = rel_path
    return cand

def should_skip_file(fname):
    base = os.path.splitext(fname)[0]
    for k in IGNORE_KEYWORDS:
        if k in base: return True
    return False

def parse_file(path, ruleset):
    try:
        with open(path, 'r', encoding='utf-8', errors='ignore') as f:
            if path.endswith(('.yaml', '.yml')):
                try:
                    data = yaml.safe_load(f)
                    if data and 'payload' in data:
                        for l in data['payload']: process_entry(str(l), ruleset)
                except: pass
            else:
                for l in f: process_entry(l, ruleset)
    except: pass

def process_entry(line, ruleset):
    line = line.strip()
    if not line or line.startswith('#'): return
    if line.startswith("['"): line = line.replace('[','').replace(']','').replace("'", "")
    upper = line.upper()
    if 'DOMAIN' in upper or (not 'IP-' in upper and '.' in line and not line[0].isdigit()):
        ruleset.add_domain(line)
    else:
        ruleset.add_ip(line)

def build_mihomo(kernel, name, ruleset):
    h_d, h_i = False, False
    
    if ruleset.domain_entries:
        final_domains = set()
        raw_candidates = set()

        for t, v in ruleset.domain_entries:
            if 'KEYWORD' in t.upper():
                for kw, domains in KEYWORD_RESCUE_MAP.items():
                    if kw in v.lower():
                        for d in domains:
                            raw_candidates.add(d)
                continue 

            if 'REGEX' in t.upper(): continue
            raw_candidates.add(v)
        
        for d in raw_candidates:
            if d.startswith('.'):
                clean_d = d[1:] 
                final_domains.add(clean_d)
                final_domains.add(d) 
            else:
                final_domains.add(d)
                final_domains.add(f".{d}")

        clean = sorted(list(final_domains))
        
        if clean and _compile_mihomo(kernel, name, clean, 'domain'): 
            h_d = True
            
    if ruleset.ip_entries:
        clean = sorted(ruleset.ip_entries.keys())
        if _compile_mihomo(kernel, f"{name}_IP", clean, 'ipcidr'): h_i = True
        
    return h_d, h_i

def _compile_mihomo(kernel, name, rules, behavior):
    tmp = f"temp_{name}.yaml"
    dst = os.path.join(TARGET_DIR_MIHOMO, f"{name}.mrs")
    try:
        with open(tmp, 'w', encoding='utf-8') as f: yaml.dump({'payload': rules}, f)
        res = subprocess.run(kernel.get_cmd(behavior, tmp, dst), capture_output=True, text=True, timeout=20)
        if res.returncode != 0 or os.path.getsize(dst) == 0:
            if os.path.exists(dst): os.remove(dst)
            return False
        return True
    except: return False
    finally:
        if os.path.exists(tmp): os.remove(tmp)

def build_loon(name, ruleset):
    count = len(ruleset.domain_entries) + len(ruleset.ip_entries)
    if count == 0: return False
    dst = os.path.join(TARGET_DIR_LOON, f"{name}.lsr")
    lines = []
    for ip, no_res in ruleset.ip_entries.items():
        lines.append(f"IP-CIDR,{ip}{',no-resolve' if no_res else ''}")
    for t, v in ruleset.domain_entries: lines.append(f"{t},{v}")
    lines.sort()
    lines.sort(key=lambda x: 0 if "no-resolve" in x else (1 if "DOMAIN" in x else 2))
    bj_time = (datetime.now(timezone.utc) + timedelta(hours=8)).strftime('%Y-%m-%d %H:%M:%S')
    try:
        with open(dst, 'w', encoding='utf-8') as f:
            f.write(f"# Name = {name}\n# Author = {AUTHOR_NAME}\n# REPO = {REPO_URL}\n# Update = {bj_time}\n# Total = {count}\n\n")
            f.write("\n".join(lines))
        return True
    except: return False

def get_status_text(days):
    if days == 0: return "Today"
    if days == 1: return "Yesterday"
    return f"{days} days ago"

def detect_config_file():
    try:
        files = os.listdir('.')
    except:
        return "Mihomo_ShuntRules.yaml", False
    for f in files:
        if f.endswith(('.yaml', '.yml')) and "Mihomo" in f and "Shunt" in f:
            return f, True
    for f in files:
         if f.endswith(('.yaml', '.yml')) and ("Mihomo" in f or "Config" in f):
            return f, True
    return "Mihomo_ShuntRules.yaml", False

def generate_readme(stats):
    stats.sort(key=lambda x: x[0])
    total = len(stats)
    # ğŸŸ¢ ä¿®å¤ Bug: ä½¿ç”¨ç‚¹å·åˆ†å‰²æ—¥æœŸï¼Œä¿®å¤ Shields.io 404
    bj_time = (datetime.now(timezone.utc) + timedelta(hours=8)).strftime('%Y.%m.%d') 
    time_badge_val = bj_time
    
    config_name, found = detect_config_file()
    config_link = f"[{config_name}]({RAW_BASE_URL}/{config_name})"

    # âœ… å¾½ç« ç”ŸæˆåŒº (çº¯è‰²åœ†è§’é£æ ¼ï¼ŒæŒ‰è¦æ±‚æ’åº)
    badges = [
        f"![Total](https://img.shields.io/badge/-è§„åˆ™æ€»æ•°%20{total}-blue?style=flat)", 
        f"![Update](https://img.shields.io/badge/-æ›´æ–°æ—¶é—´%20{time_badge_val}-2ea44f?style=flat)",
        f"![Dedupe](https://img.shields.io/badge/-å»é‡å¤„ç†-607d8b?style=flat)",
        f"![Anchor](https://img.shields.io/badge/-åŒé‡é”šå®š-8e44ad?style=flat)",
        f"![Rescue](https://img.shields.io/badge/-å…³é”®è¯è½¬è¯‘-e67e22?style=flat)",
        f"![Sort](https://img.shields.io/badge/-æ’åºä¼˜åŒ–-009688?style=flat)",
        f"![Format](https://img.shields.io/badge/-æ ¼å¼æ”¯æŒ%20MRS%20&%20LSR-003366?style=flat)",
        f"![Ready](https://img.shields.io/badge/-å¼€ç®±å³ç”¨-ff69b4?style=flat)"
    ]
    badge_line = " ".join(badges)

    md = [
        # âœ… æ ‡é¢˜ä¸å¾½ç« å±…ä¸­
        f"<div align=\"center\">",
        f"",
        f"# ğŸ¤– Auto Shunt Rules", 
        f"",
        f"{badge_line}",
        f"",
        f"</div>",
        f"",
        f"## â„¹ï¸ æ•°æ®æºè¯´æ˜",
        f"â™»ï¸ æœ¬ä»“åº“è§„åˆ™æ•°æ®åŒæ­¥è‡ª [blackmatrix7/ios_rule_script](https://github.com/blackmatrix7/ios_rule_script) é¡¹ç›®ï¼Œæ„Ÿè°¢å„ä½ç»´æŠ¤è§„åˆ™çš„å¤§ä½¬ä»¬ã€‚",
        f"",
        f"## âš ï¸ ä½¿ç”¨å‰å¿…è¯»",
        f"* ğŸ± Mihomo: .mrs äºŒè¿›åˆ¶æ ¼å¼ã€‚é‡‡ç”¨åŒé‡é”šå®šç­–ç•¥ï¼Œè§£å†³å­åŸŸåæ¼ç½‘ä¸è§†é¢‘æµåŒ¹é…éš¾é¢˜ã€‚_IP.mrs å·²ç§»é™¤ `no-resolve` å‚æ•°ã€‚",
        f"* ğŸˆ Loon: .lsr æ–‡æœ¬æ ¼å¼ã€‚æ”¯æŒæ··åˆè´Ÿè½½å¹¶ä¼˜åŒ–æ’åºï¼ˆ`no-resolve IP` ä¼˜å…ˆï¼‰ï¼Œç¡®ä¿åŒ¹é…æ•ˆç‡å¹¶é˜²æ­¢ DNS æ³„éœ²ã€‚",
        f"* ğŸ­ DNS æ³„éœ²: IP è§„åˆ™åœ¨åŒ¹é…å‰å¿…é¡»å…ˆè§£æåŸŸåï¼Œè€Œè§£æè¿‡ç¨‹ä¼šä½¿ç”¨ DNS é…ç½®ä¸­çš„ `nameserver` å­—æ®µæŒ‡å®šçš„æœåŠ¡å™¨ã€‚è¿™å¯èƒ½ä¼šæš´éœ²è®¿é—®ç›®æ ‡ï¼Œå¦‚éœ€ä½¿ç”¨ IP è§„åˆ™å»ºè®®æ·»åŠ  `no-resolve` å‚æ•°ã€‚",
        f"",
        f"## ğŸ“ Mihomo é…ç½®æŒ‡å¼•",
        f"> âš¡ ä½¿ç”¨æ–¹å¼: ç”¨ `type: http` è¿œç¨‹å¼•ç”¨è§„åˆ™é›†ã€‚",
        f"> ğŸ”— è¦†å†™å‚è€ƒ: {config_link}",
        f"",
        # âœ… ä»£ç æŠ˜å  + æ–‡æ¡ˆå¾®è°ƒ
        f"<details>",
        f"<summary><strong>ğŸ’¾ é…ç½®ç¤ºä¾‹ <sub>(ä»¥ Google ä¸ºä¾‹ï¼Œç‚¹å‡»å±•å¼€)</sub></strong></summary>",
        f"",
        f"### 1. å®šä¹‰ç­–ç•¥ç»„ (Proxy Groups)",
        f"```yaml",
        f"proxy-groups:",
        f"  - name: \"MyProxyGroup\"   # ç­–ç•¥ç»„åç§°ï¼Œå¯è‡ªå®šä¹‰",
        f"    type: select",
        f"    proxies:",
        f"      - ğŸ‡­ğŸ‡° é¦™æ¸¯èŠ‚ç‚¹      # ğŸ‘ˆ è¿™é‡Œå¡«å†™ä½ åœ¨ 'proxies' ä¸­å®šä¹‰çš„èŠ‚ç‚¹åç§°",
        f"      - ğŸ‡ºğŸ‡¸ ç¾å›½èŠ‚ç‚¹      # ğŸ‘ˆ æˆ–è€…å¡«å†™ 'DIRECT' (ç›´è¿) / 'REJECT' (æ‹’ç»)",
        f"```",
        f"",
        f"### 2. é…ç½®è§„åˆ™é›† (Rule Providers)",
        f"```yaml",
        f"rule-providers:",
        f"  # ğŸŸ¢ æ¡ˆä¾‹ 1ï¼šå¼•ç”¨åŸŸåè§„åˆ™ (behavior: domain)",
        f"  Google:",
        f"    type: http",
        f"    behavior: domain",
        f"    format: mrs",
        f"    url: \"{RAW_BASE_URL}/{TARGET_DIR_MIHOMO}/Google.mrs\"",
        f"    path: ./rules/Mihomo/Google.mrs",
        f"    interval: 86400",
        f"",
        f"  # ğŸŸ¢ æ¡ˆä¾‹ 2ï¼šå¼•ç”¨ IP è§„åˆ™ (behavior: ipcidr)",
        f"  Google_IP:",
        f"    type: http",
        f"    behavior: ipcidr",
        f"    format: mrs",
        f"    url: \"{RAW_BASE_URL}/{TARGET_DIR_MIHOMO}/Google_IP.mrs\"",
        f"    path: ./rules/Mihomo/Google_IP.mrs",
        f"    interval: 86400",
        f"```",
        f"",
        f"### 3. åº”ç”¨è§„åˆ™ (Rules)",
        f"*âš ï¸ å…³é”®ï¼šå¼•ç”¨ IP è§„åˆ™é›†æ—¶ï¼Œå»ºè®®åŠ ä¸Š `no-resolve`ï¼Œé˜²æ­¢ DNS æ³„éœ²ã€‚*",
        f"```yaml",
        f"rules:",
        f"  - RULE-SET,Google,MyProxyGroup",
        f"  - RULE-SET,Google_IP,MyProxyGroup,no-resolve",
        f"```",
        f"</details>",
        f"",
        f"## ğŸ“Š è§„åˆ™ç´¢å¼•",
        f"| è§„åˆ™åç§° | Mihomo (.mrs) | Loon (.lsr) | æ›´æ–°çŠ¶æ€ |",
        f"| :---: | :---: | :---: | :---: |"
    ]
    
    for name, status, has_d, has_i, has_l in stats:
        mihomo_links = []
        if has_d: mihomo_links.append(f"[`DOMAIN`]({RAW_BASE_URL}/{TARGET_DIR_MIHOMO}/{name}.mrs)")
        if has_i: mihomo_links.append(f"[`IP-CIDR`]({RAW_BASE_URL}/{TARGET_DIR_MIHOMO}/{name}_IP.mrs)")
        m_cell = " \\| ".join(mihomo_links) if mihomo_links else "-"
        l_cell = f"[`RAW Link`]({RAW_BASE_URL}/{TARGET_DIR_LOON}/{name}.lsr)" if has_l else "-"
        md.append(f"| {name} | {m_cell} | {l_cell} | {status} |")
        
    with open(README_FILE, 'w', encoding='utf-8') as f: f.write("\n".join(md))

def main():
    for d in [TARGET_DIR_MIHOMO, TARGET_DIR_LOON]:
        os.makedirs(d, exist_ok=True)
    if not os.path.exists(SOURCE_DIR): return
    kernel, history, aggregated = KernelIntrospector(MIHOMO_BIN), HistoryManager(), defaultdict(RuleSet)
    logger.info("ğŸ” æ‰«ææºæ–‡ä»¶...")
    
    rel_path_map = {} 
    cnt, skip = 0, 0
    for root, _, files in os.walk(SOURCE_DIR):
        rel = os.path.relpath(root, SOURCE_DIR)
        if rel == '.': continue
        rs = aggregated[rel]
        for f in files:
            if f.lower().endswith(('.yaml','.yml','.list','.txt')) and not should_skip_file(f):
                full_path = os.path.join(root, f)
                parse_file(full_path, rs)
                rel_path_map[rel] = full_path 
                cnt += 1
                
    logger.info(f"âœ… è§£æå®Œæˆã€‚è§„åˆ™ç»„: {len(aggregated)}")
    
    stats = []
    valid_outputs = set()
    
    updated_count = 0
    skipped_count = 0
    
    # ğŸŸ¢ æ ¸å¿ƒä¿®å¤ï¼šæŒ‰è·¯å¾„æ·±åº¦æ’åºï¼Œä¼˜å…ˆå¤„ç†æµ…å±‚ç›®å½•ï¼Œé˜²æ­¢æ·±å±‚ç›®å½•æŠ¢å ç®€çŸ­æ–‡ä»¶å
    # lambda x: (x.count(os.sep), x) è¡¨ç¤ºå…ˆæŒ‰åˆ†éš”ç¬¦æ•°é‡(æ·±åº¦)æ’ï¼Œå†æŒ‰åç§°æ’
    sorted_rels = sorted(aggregated.keys(), key=lambda x: (x.count(os.sep), x))
    
    for rel in sorted_rels:
        rs = aggregated[rel]
        name = get_smart_filename(rel)
        source_path = rel_path_map.get(rel)
        if not source_path: continue

        expect_d = bool(rs.domain_entries)
        expect_i = bool(rs.ip_entries)
        expect_l = expect_d or expect_i
        
        expected_files = []
        if expect_d: expected_files.append(os.path.join(TARGET_DIR_MIHOMO, f"{name}.mrs"))
        if expect_i: expected_files.append(os.path.join(TARGET_DIR_MIHOMO, f"{name}_IP.mrs"))
        if expect_l: expected_files.append(os.path.join(TARGET_DIR_LOON, f"{name}.lsr"))
        
        should_skip_build, src_hash = history.should_skip(name, source_path, expected_files)
        
        h_d, h_i, h_l = expect_d, expect_i, expect_l
        
        if should_skip_build:
            skipped_count += 1
            days = history.get_days_ago(name)
        else:
            updated_count += 1
            h_d, h_i = build_mihomo(kernel, name, rs)
            h_l = build_loon(name, rs)
            history.update_record(name, src_hash)
            days = 0
            
        if h_d: valid_outputs.add(os.path.join(TARGET_DIR_MIHOMO, f"{name}.mrs"))
        if h_i: valid_outputs.add(os.path.join(TARGET_DIR_MIHOMO, f"{name}_IP.mrs"))
        if h_l: valid_outputs.add(os.path.join(TARGET_DIR_LOON, f"{name}.lsr"))
        
        if h_d or h_i or h_l:
            stats.append((name, get_status_text(days), h_d, h_i, h_l))
            
    logger.info("ğŸ§¹ æ‰§è¡Œæ¸…ç†...")
    removed_zombies = 0
    for d in [TARGET_DIR_MIHOMO, TARGET_DIR_LOON]:
        if not os.path.exists(d): continue
        for f in os.listdir(d):
            full_p = os.path.join(d, f)
            if full_p not in valid_outputs:
                os.remove(full_p)
                removed_zombies += 1
                
    history.save()
    generate_readme(stats)
    logger.info(f"ğŸ‰ å®Œæˆ: æ›´æ–° {updated_count}, è·³è¿‡ {skipped_count}, æ¸…ç† {removed_zombies}")

if __name__ == "__main__":
    main()
